% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mutual.information.R
\name{mutual.information}
\alias{mutual.information}
\title{Calculate mutual information between two vectors of data}
\usage{
mutual.information(
  data.1,
  data.2,
  data.1.categorical = FALSE,
  data.2.categorical = FALSE,
  n.bins,
  FD.n.bins = FALSE,
  equal.frequency.bins = FALSE,
  normalize.mutual.info = TRUE,
  use.shrinkage = FALSE,
  simple.sample.size.correction = FALSE
)
}
\arguments{
\item{data.1}{first input vector from which to calculate mutual information}

\item{data.2}{second input vector from which to calculate mutual information}

\item{data.1.categorical}{does data.1 represent a categorical variable. Used if FD.n.bins is TRUE. If so, the number of bins used is equal to the number of unique values of data.1.
data.1 must be numeric for discretization, therefore categorical data must still be encoded in numeric form. Defaults to FALSE.}

\item{data.2.categorical}{does data.1 represent a categorical variable. Used if FD.n.bins is TRUE. If so, the number of bins used is equal to the number of unique values of data.1.
data.1 must be numeric for discretization, therefore categorical data must still be encoded in numeric form. Defaults to FALSE.}

\item{n.bins}{number of bins to use in discretizing data, applied to both input vectors. Used if FD.n.bins = FALSE}

\item{FD.n.bins}{whether to calculate number of bins for discretization based upon input data using the Freedmanâ€“Diaconis rule.
Performed for both input vectors separately. Overides n.bins and sample.size.bins.}

\item{equal.frequency.bins}{whether to use an adaptive bin width which aims to equalize the frequencies of observations in each bin. Defaults to FALSE}

\item{normalize.mutual.info}{should the mutual information be normalized (bounded between 0-1). Defaults to TRUE.}

\item{use.shrinkage}{whether to perform shrinkage when calculating mutual information using the James-Stein estimator. Defaults to FALSE}

\item{simple.sample.size.correction}{apply a naive correction to reduce the bias coming from a limited sample size}
}
\value{
mutual information (after optional normalization)
}
\description{
Perform discretization and calculation of mutual information using functions from entropy library
}
